{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c738dda",
   "metadata": {},
   "source": [
    "# Loading NOAA Locations Data into DataFrames\n",
    "\n",
    "This notebook demonstrates how to load and work with NOAA location data from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69c90c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for data in: /Users/iara/Projects/Week5JupyterNotebooks/DataAcqusitionLab/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the data directory\n",
    "DATA_DIR = '../DataAcqusitionLab/data/'\n",
    "print(f\"Looking for data in: {os.path.abspath(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e6bbc",
   "metadata": {},
   "source": [
    "## 1. Examine Directory Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5d4ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 JSON files in ../DataAcqusitionLab/data/\n"
     ]
    }
   ],
   "source": [
    "# List all JSON files in the data directory\n",
    "json_files = glob.glob(os.path.join(DATA_DIR, 'data/locations_*.json'))\n",
    "json_files.sort()\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files in {DATA_DIR}\")\n",
    "if json_files:\n",
    "    print(f\"First file: {os.path.basename(json_files[0])}\")\n",
    "    print(f\"Last file: {os.path.basename(json_files[-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea4666",
   "metadata": {},
   "source": [
    "## 2. Examine a Single JSON File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0c2a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the first JSON file\n",
    "if json_files:\n",
    "    sample_file = json_files[0]\n",
    "    print(f\"Examining: {os.path.basename(sample_file)}\")\n",
    "    \n",
    "    with open(sample_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nJSON structure:\")\n",
    "    print(f\"Keys: {list(data.keys())}\")\n",
    "    \n",
    "    if 'results' in data:\n",
    "        results = data['results']\n",
    "        print(f\"Number of records in this file: {len(results)}\")\n",
    "        print(f\"\\nSample record:\")\n",
    "        print(results[0] if results else \"No records found\")\n",
    "    \n",
    "    if 'metadata' in data:\n",
    "        metadata = data['metadata']\n",
    "        print(f\"\\nMetadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b9ba9",
   "metadata": {},
   "source": [
    "## 3. Load All JSON Files and Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae75b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 39 JSON files...\n",
      "  Loaded 1/39 files, 1000 total records so far\n",
      "  Loaded 10/39 files, 10000 total records so far\n",
      "  Loaded 20/39 files, 20000 total records so far\n",
      "  Loaded 30/39 files, 30000 total records so far\n",
      "  Loaded 39/39 files, 38862 total records so far\n",
      "\n",
      "Completed loading 39 files with 38862 total records\n"
     ]
    }
   ],
   "source": [
    "# Function to load and combine all JSON files\n",
    "def load_all_location_data(data_dir):\n",
    "    \"\"\"Load all location JSON files and combine into a single list.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(data_dir, 'locations_*.json'))\n",
    "    json_files.sort()\n",
    "    \n",
    "    all_results = []\n",
    "    total_files = len(json_files)\n",
    "    \n",
    "    print(f\"Loading {total_files} JSON files...\")\n",
    "    \n",
    "    for i, file_path in enumerate(json_files):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if 'results' in data:\n",
    "                results = data['results']\n",
    "                all_results.extend(results)\n",
    "                if i == 0 or (i + 1) % 10 == 0 or i == total_files - 1:\n",
    "                    print(f\"  Loaded {i+1}/{total_files} files, {len(all_results)} total records so far\")\n",
    "            else:\n",
    "                print(f\"  Warning: No 'results' key in {os.path.basename(file_path)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    print(f\"\\nCompleted loading {total_files} files with {len(all_results)} total records\")\n",
    "    return all_results\n",
    "\n",
    "# Load all the data\n",
    "all_location_data = load_all_location_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a084a52",
   "metadata": {},
   "source": [
    "## 4. Create DataFrame from Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4caea92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created successfully!\n",
      "Shape: (38862, 5)\n",
      "Columns: ['mindate', 'maxdate', 'name', 'datacoverage', 'id']\n",
      "\n",
      "Data types:\n",
      "mindate          object\n",
      "maxdate          object\n",
      "name             object\n",
      "datacoverage    float64\n",
      "id               object\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mindate</th>\n",
       "      <th>maxdate</th>\n",
       "      <th>name</th>\n",
       "      <th>datacoverage</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1983-01-01</td>\n",
       "      <td>2025-08-03</td>\n",
       "      <td>Abu Dhabi, AE</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>CITY:AE000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>2025-08-03</td>\n",
       "      <td>Ajman, AE</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>CITY:AE000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>2025-08-03</td>\n",
       "      <td>Dubai, AE</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>CITY:AE000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>2025-08-03</td>\n",
       "      <td>Sharjah, AE</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>CITY:AE000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1966-03-02</td>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>Kabul, AF</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>CITY:AF000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mindate     maxdate           name  datacoverage             id\n",
       "0  1983-01-01  2025-08-03  Abu Dhabi, AE        0.9970  CITY:AE000001\n",
       "1  1944-03-01  2025-08-03      Ajman, AE        1.0000  CITY:AE000002\n",
       "2  1944-03-01  2025-08-03      Dubai, AE        1.0000  CITY:AE000003\n",
       "3  1944-03-01  2025-08-03    Sharjah, AE        1.0000  CITY:AE000006\n",
       "4  1966-03-02  2021-08-30      Kabul, AF        0.9969  CITY:AF000007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame from the combined data\n",
    "if all_location_data:\n",
    "    df_locations = pd.DataFrame(all_location_data)\n",
    "    \n",
    "    print(f\"DataFrame created successfully!\")\n",
    "    print(f\"Shape: {df_locations.shape}\")\n",
    "    print(f\"Columns: {list(df_locations.columns)}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df_locations.dtypes)\n",
    "    \n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(df_locations.head())\n",
    "else:\n",
    "    print(\"No data loaded - cannot create DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b164592",
   "metadata": {},
   "source": [
    "## 5. Data Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f51296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NOAA Locations Data Summary ===\n",
      "Total locations: 38,862\n",
      "Date range: 1750-08-06 to 2025-08-05\n",
      "\n",
      "=== Location Types ===\n",
      "0\n",
      "ZIP     30415\n",
      "FIPS     3438\n",
      "HUC      2667\n",
      "CITY     1989\n",
      "CLIM      353\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Data Coverage Statistics ===\n",
      "count    38862.000000\n",
      "mean         0.985740\n",
      "std          0.030188\n",
      "min          0.075800\n",
      "25%          0.950000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: datacoverage, dtype: float64\n",
      "\n",
      "=== Sample Locations by Type ===\n",
      "ZIP: Highgate Center, VT 05459 (ZIP:05459)\n",
      "FIPS: Alabama (FIPS:01)\n",
      "HUC: New England Hydrologic Unit (HUC:01)\n"
     ]
    }
   ],
   "source": [
    "# Basic data analysis\n",
    "if 'df_locations' in locals() and not df_locations.empty:\n",
    "    print(\"=== NOAA Locations Data Summary ===\")\n",
    "    print(f\"Total locations: {len(df_locations):,}\")\n",
    "    print(f\"Date range: {df_locations['mindate'].min()} to {df_locations['maxdate'].max()}\")\n",
    "    \n",
    "    # Analyze location types by ID prefix\n",
    "    print(f\"\\n=== Location Types ===\")\n",
    "    location_types = df_locations['id'].str.split(':', expand=True)[0].value_counts()\n",
    "    print(location_types)\n",
    "    \n",
    "    # Data coverage statistics\n",
    "    print(f\"\\n=== Data Coverage Statistics ===\")\n",
    "    print(df_locations['datacoverage'].describe())\n",
    "    \n",
    "    # Sample of different location types\n",
    "    print(f\"\\n=== Sample Locations by Type ===\")\n",
    "    for loc_type in location_types.index[:3]:  # Show top 3 types\n",
    "        sample = df_locations[df_locations['id'].str.startswith(loc_type + ':')].iloc[0]\n",
    "        print(f\"{loc_type}: {sample['name']} ({sample['id']})\")\n",
    "else:\n",
    "    print(\"DataFrame not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfacba",
   "metadata": {},
   "source": [
    "## 6. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7b59f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Validation ===\n",
      "Missing values per column:\n",
      "mindate         0\n",
      "maxdate         0\n",
      "name            0\n",
      "datacoverage    0\n",
      "id              0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "Unique IDs: 38,862\n",
      "Total rows: 38,862\n",
      "ID uniqueness: ✓ All unique\n",
      "\n",
      "=== Record Count Validation ===\n",
      "Expected records: 38,862\n",
      "Actual records: 38,862\n",
      "Status: ✓ Match\n"
     ]
    }
   ],
   "source": [
    "# Validate the data\n",
    "if 'df_locations' in locals() and not df_locations.empty:\n",
    "    print(\"=== Data Validation ===\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = df_locations.isnull().sum()\n",
    "    print(f\"Missing values per column:\")\n",
    "    print(missing_counts)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df_locations.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicate_count}\")\n",
    "    \n",
    "    # Check unique IDs\n",
    "    unique_ids = df_locations['id'].nunique()\n",
    "    total_rows = len(df_locations)\n",
    "    print(f\"Unique IDs: {unique_ids:,}\")\n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    print(f\"ID uniqueness: {'✓ All unique' if unique_ids == total_rows else '⚠ Duplicates found'}\")\n",
    "    \n",
    "    # Expected total from NOAA API\n",
    "    expected_total = 38862  # Current known total\n",
    "    print(f\"\\n=== Record Count Validation ===\")\n",
    "    print(f\"Expected records: {expected_total:,}\")\n",
    "    print(f\"Actual records: {total_rows:,}\")\n",
    "    print(f\"Status: {'✓ Match' if total_rows == expected_total else '⚠ Count mismatch'}\")\n",
    "else:\n",
    "    print(\"DataFrame not available for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa5524",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully loads NOAA location data from JSON files into a pandas DataFrame for analysis. The data includes various location types (ZIP codes, FIPS codes, weather stations, etc.) with their associated date ranges and data coverage information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
